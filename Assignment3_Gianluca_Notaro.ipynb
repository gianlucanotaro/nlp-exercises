{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagger  Gianluca Notaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_stts(text):\n",
    "    text = ''.join(text)\n",
    "    splits = text.split(';')\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    for i in splits:\n",
    "        i = i.split('/')\n",
    "        if len(i)==2: #Discard last \"Word\" with no tag\n",
    "            if i[0][:1] == '[':\n",
    "                i[0] = i[0][2:]# remove [' from first word\n",
    "            sentences.append(i[0].replace(' ', ''))\n",
    "            tags.append(i[1].replace(' ',''))\n",
    "    return [sentences,tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding(tagslist):\n",
    "    cleaned_tags = []\n",
    "    for t in range(len(tagslist)):\n",
    "        if tagslist[t] == '-PAD-':\n",
    "            pass\n",
    "        else:\n",
    "            cleaned_tags.append(tagslist[t])\n",
    "    return cleaned_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    "        token_sequences.append(token_sequence)\n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_flattener(taglist):\n",
    "    flat_list = []\n",
    "    for sublist in taglist:\n",
    "        for tag in sublist:\n",
    "            flat_list.append(tag)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(pred, actual):\n",
    "    pred = list_flattener(pred)\n",
    "    actual = list_flattener(actual)\n",
    "    count = len(actual)\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if pred[i] == actual[i]:\n",
    "            correct = correct +  1\n",
    "    print('Accuracy: ', (correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stts_to_uPOS(tag):\n",
    "    switcher = {\n",
    "    'ADJA': 'ADJ',\n",
    "    'ADJD': 'ADJ',\n",
    "    'ADV': 'ADV',\n",
    "    'APPR': 'ADP',\n",
    "    'APPRART': 'ADP',\n",
    "    'APPO': 'ADP',\n",
    "    'APZR': 'ADP',\n",
    "    'ART': 'DET',\n",
    "    'CARD': 'NUM',\n",
    "    'FM': 'X',\n",
    "    'ITJ': 'INTJ',\n",
    "    'KOUI': 'SCONJ',\n",
    "    'KOUS': 'SCONJ',\n",
    "    'KON': 'CCONJ',\n",
    "    'KOKOM': 'CCONJ',\n",
    "    'NN': 'NOUN',\n",
    "    'NE': 'PROPN',\n",
    "    'PDS': 'PRON',\n",
    "    'PDAT': 'DET',\n",
    "    'PIS': 'PRON',\n",
    "    'PIAT': 'DET',\n",
    "    'PIDAT': 'DET',\n",
    "    'PPER': 'PRON',\n",
    "    'PPOSS': 'PRON',\n",
    "    'PPOSAT': 'DET',\n",
    "    'PRELS': 'PRON',\n",
    "    'PRELAT': 'DET',\n",
    "    'PRF': 'PRON',\n",
    "    'PWS': 'PRON',\n",
    "    'PWAT': 'DET',\n",
    "    'PWAV': 'DET',\n",
    "    'PAV': 'ADV',\n",
    "    'PTKZU': 'PART',\n",
    "    'PTKNEG': 'PART',\n",
    "    'PTKVZ': 'ADP',\n",
    "    'PTKANT': 'PART',\n",
    "    'PTKA': 'PART',\n",
    "    'VVFIN': 'VERB',\n",
    "    'VVIMP': 'VERB',\n",
    "    'VVINF': 'VERB',\n",
    "    'VVIZU': 'VERB',\n",
    "    'VVPP': 'VERB',\n",
    "    'VAFIN': 'AUX',\n",
    "    'VAIMP': 'AUX',\n",
    "    'VAINF': 'AUX',\n",
    "    'VAPP': 'AUX',\n",
    "    'VMFIN': 'VERB',\n",
    "    'VMINF': 'VERB',\n",
    "    'VMPP': 'VERB',\n",
    "    'XY': 'X',\n",
    "    'SGML': 'X',\n",
    "    'SPELL': 'X',\n",
    "    'TRUNC': 'X',\n",
    "    '$,': 'PUNCT',\n",
    "    '$.': 'PUNCT',\n",
    "    '$(': 'PUNCT'\n",
    "    }\n",
    "    return switcher.get(tag,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einlesen und vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = pd.read_csv('POS_German_train.txt', delimiter='\\t', header=None)\n",
    "pos_train = pos_train.values.tolist()\n",
    "\n",
    "pos_test = pd.read_csv('POS_German_minitest.txt', delimiter='\\t', header=None)\n",
    "pos_test = pos_test.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitten in tags  und Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "tags = []\n",
    "lister = []\n",
    "\n",
    "N = len(pos_train)\n",
    "for i in range(N):\n",
    "    lister.append(convert_to_stts(pos_train[i]))\n",
    "\n",
    "for i in range(N):\n",
    "    sentences.append(lister[i][0])\n",
    "    tags.append(lister[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags umwandeln in POS-Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uPOS_tags  = []\n",
    "\n",
    "for i in  tags:\n",
    "    uPOS_tags.append(list(map(stts_to_uPOS,i)))\n",
    "\n",
    "(words_train, words_test, tags_train, tags_test) = train_test_split(sentences, uPOS_tags, test_size=0.0)\n",
    "\n",
    "words,tags = set([]), set([])\n",
    "\n",
    "for s in words_train:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "        \n",
    "for ts in tags_train:\n",
    " for t  in ts:\n",
    "        tags.add(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow kann nicht mit Strings arbeiten, darum muss jedes Wort/Tag umgewandelt werden in einen integer Wert. Um mit Keras arbeiten zu können braucht es Sequenzen mit gleicher Länge, darum werden die Listen unten mit -PAD- auf gleihcer Länge gebracht. -OOV- steht für Out of word und wird beim testen gebraucht, falls ein Wort vorkommt der nicht im indexer ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndexer = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "wordIndexer['-PAD-'] = 0  # The special value used for padding\n",
    "wordIndexer['-OOV-'] = 1  # The special value used for OOVs\n",
    "\n",
    "tagIndexer = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tagIndexer['-PAD-'] = 0  # The special value used to padding\n",
    "        \n",
    "words_train_x, tags_train_y, words_minitest_x, tags_minitest_y = [],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in words_train:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(wordIndexer[w.lower()])\n",
    "        except:\n",
    "            s_int.append(wordIndexer['-OOV-'])\n",
    "    words_train_x.append(s_int)\n",
    "    \n",
    "for s in tags_train:\n",
    "    tags_train_y.append([tagIndexer[t] for t in s])\n",
    "    \n",
    "    \n",
    "    \n",
    "lister_test = []\n",
    "sentences_minitest = []\n",
    "tags_minitest = []\n",
    "\n",
    "M = len(pos_test)\n",
    "for i in range(M):\n",
    "    lister_test.append(convert_to_stts(pos_test[i]))\n",
    "\n",
    "for i in range(M):\n",
    "    sentences_minitest.append(lister_test[i][0])\n",
    "    tags_minitest.append(lister_test[i][1])\n",
    "    \n",
    "words_minitest_x = []\n",
    "for s in sentences_minitest:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(wordIndexer[w.lower()])\n",
    "        except:\n",
    "            s_int.append(wordIndexer['-OOV-'])\n",
    "    words_minitest_x.append(s_int)\n",
    "\n",
    "tags_minitest_pos = []\n",
    "\n",
    "for i in tags_minitest:\n",
    "    tags_minitest_pos.append(list(map(stts_to_uPOS,i)))\n",
    "tags_minitest_pos_y = []\n",
    "\n",
    "for i in tags_minitest_pos:\n",
    "      tags_minitest_pos_y.append([tagIndexer[t] for t in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = len(max(words_train))\n",
    "words_train_x = pad_sequences(words_train_x, maxlen=MAX_LEN, padding='post')\n",
    "tags_train_y = pad_sequences(tags_train_y,maxlen=MAX_LEN, padding='post')\n",
    "words_minitest_x = pad_sequences(words_minitest_x, maxlen=MAX_LEN, padding='post')\n",
    "tags_minitest_pos_y = pad_sequences(tags_minitest_pos_y, maxlen=MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model vorbereiten und trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 128)           9443328   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 18, 512)           788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 18, 17)            8721      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 18, 17)            0         \n",
      "=================================================================\n",
      "Total params: 10,240,529\n",
      "Trainable params: 10,240,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LEN, )))\n",
    "model.add(Embedding(len(wordIndexer),128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tagIndexer))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(0.001),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Input hier um den Model zu trainieren ist in One-Hot Encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22284 samples, validate on 14856 samples\n",
      "Epoch 1/10\n",
      "22284/22284 [==============================] - 41s 2ms/step - loss: 0.0176 - acc: 0.9958 - val_loss: 0.1274 - val_acc: 0.9620\n",
      "Epoch 2/10\n",
      "22284/22284 [==============================] - 41s 2ms/step - loss: 0.0143 - acc: 0.9968 - val_loss: 0.1323 - val_acc: 0.9614\n",
      "Epoch 3/10\n",
      "22284/22284 [==============================] - 42s 2ms/step - loss: 0.0117 - acc: 0.9975 - val_loss: 0.1311 - val_acc: 0.9613\n",
      "Epoch 4/10\n",
      "22284/22284 [==============================] - 42s 2ms/step - loss: 0.0093 - acc: 0.9982 - val_loss: 0.1341 - val_acc: 0.9621\n",
      "Epoch 5/10\n",
      "22284/22284 [==============================] - 42s 2ms/step - loss: 0.0076 - acc: 0.9987 - val_loss: 0.1369 - val_acc: 0.9614\n",
      "Epoch 6/10\n",
      "22284/22284 [==============================] - 41s 2ms/step - loss: 0.0061 - acc: 0.9991 - val_loss: 0.1418 - val_acc: 0.9617\n",
      "Epoch 7/10\n",
      "22284/22284 [==============================] - 42s 2ms/step - loss: 0.0053 - acc: 0.9992 - val_loss: 0.1404 - val_acc: 0.9620\n",
      "Epoch 8/10\n",
      "22284/22284 [==============================] - 41s 2ms/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.1443 - val_acc: 0.9616\n",
      "Epoch 9/10\n",
      "22284/22284 [==============================] - 42s 2ms/step - loss: 0.0038 - acc: 0.9995 - val_loss: 0.1469 - val_acc: 0.9617\n",
      "Epoch 10/10\n",
      "22284/22284 [==============================] - 42s 2ms/step - loss: 0.0032 - acc: 0.9996 - val_loss: 0.1492 - val_acc: 0.9613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f702fa85a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_tags_train_y = token_to_categorical(tags_train_y,len(tagIndexer))\n",
    "model.fit(words_train_x, token_to_categorical(tags_train_y,len(tagIndexer)), batch_size=500, epochs=6, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Wie man sieht, ist es met dem eigenen Validation set recht genau\n",
    " \n",
    " \n",
    " Sample Size wurde zufällig gewählt, Epochs wurde 6 genommen, weil nach 6 keine grosse Unterschiede mehr zu sehen waren, validation_split wurde der Vorschlag einer Googlesuche genommen.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Der Output vom model.predict ist eine List von Wahrscheinlichkeiten. Es ist recht schwierig, diesen Daten zu vergleichen. Darum werden sie in One-Hot Listen umgewandelt, danach in Tokens und das Padding entfernt. So können die Daten mit tags_minitest_pos_y verglichen werden und die Genauigkeit berechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(words_minitest_x)\n",
    "result = logits_to_tokens(scores, {i: t for t, i in tagIndexer.items()})\n",
    "cleaned_scores = []\n",
    "results_y = []\n",
    "for s in result:\n",
    "    cleaned_scores.append([tagIndexer[t] for t in s])\n",
    "    \n",
    "for i in cleaned_scores:\n",
    "    results_y.append(remove_padding(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mit Testfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9495056497175142\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(cleaned_scores, tags_minitest_pos_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
